{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17391165-d981-4331-995e-889ebf597a69",
   "metadata": {},
   "source": [
    "# init spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e80b04c-1e11-4ed0-8d4c-cff7e74ef435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/07/27 07:43:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/27 07:43:25 WARN TransportClientFactory: DNS resolution succeed for spark-master/172.18.0.9:7077 took 5007 ms\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .appName(\"test_hudi\")\n",
    "    .config(\"spark.driver.memory\", \"1g\")\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse/default\")\n",
    "    .config(\"spark.jars\", \"hdfs://namenode:9000/user/hive/spark_jars/hudi-spark3.4-bundle_2.12-0.15.0.jar\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.spark.HoodieSparkKryoRegistrar\")\n",
    "    .config(\"spark.executor.cores\", 3)\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .config(\"spark.executor.instances\", 2)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5526e23-746b-43d6-9fc6-26abfc1c6a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gateway:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test_hudi</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff680edb10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3193b62-c41a-4718-b931-89ff5569677e",
   "metadata": {},
   "source": [
    "# test hudi table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbe3e44-8dc4-4c70-a006-7bcd8b0ca13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "create database bizvault\n",
    "location 'hdfs://namenode:9000/user/hive/warehouse/bizvault'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ea95c94-c1da-41d2-99ba-10e226e29fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/27 09:00:46 WARN TableSchemaResolver: Could not find any data file written for commit, so could not get schema for table hdfs://namenode:9000/user/hive/warehouse/bizvault/test_hudi\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "create external table bizvault.test_hudi (key int, value string)\n",
    "using hudi\n",
    "location 'hdfs://namenode:9000/user/hive/warehouse/bizvault/test_hudi'\n",
    "tblproperties(\n",
    "    'hoodie.table.type'='mor',\n",
    "    'hoodie.table.base.file.format'='parquet',\n",
    "    'hoodie.datasource.meta.sync.enable'=true,\n",
    "    'hoodie.datasource.hive_sync.mode'='hms',\n",
    "    'hoodie.datasource.hive_sync.metastore.uris'='thrift://hive-metastore:9083',\n",
    "    'hoodie.datasource.hive_sync.database'='bizvault',\n",
    "    'hoodie.datasource.hive_sync.table'='test_hudi'\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebd914a4-4b59-4de2-aa89-ed1f505a89ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/27 09:01:00 WARN TableSchemaResolver: Could not find any data file written for commit, so could not get schema for table hdfs://namenode:9000/user/hive/warehouse/bizvault/test_hudi\n",
      "24/07/27 09:01:00 WARN TableSchemaResolver: Could not find any data file written for commit, so could not get schema for table hdfs://namenode:9000/user/hive/warehouse/bizvault/test_hudi\n",
      "24/07/27 09:01:00 WARN AutoRecordKeyGenerationUtils$: Precombine field  will be ignored with auto record key generation enabled\n",
      "24/07/27 09:01:04 WARN WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path hdfs://namenode:9000/user/hive/warehouse/bizvault/test_hudi.  Falling back to direct markers.\n",
      "24/07/27 09:01:04 WARN WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path hdfs://namenode:9000/user/hive/warehouse/bizvault/test_hudi.  Falling back to direct markers.\n",
      "24/07/27 09:01:04 WARN WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path hdfs://namenode:9000/user/hive/warehouse/bizvault/test_hudi.  Falling back to direct markers.\n",
      "24/07/27 09:01:05 WARN TaskSetManager: Lost task 0.0 in stage 107.0 (TID 1296) (172.18.0.12 executor 0): org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:344)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$mapPartitionsAsRDD$a3ab3c4$1(BaseSparkCommitActionExecutor.java:254)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1552)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.hudi.exception.HoodieAppendException: Failed while appending records to hdfs://namenode:9000/user/hive/warehouse/bizvault/test_hudi/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0\n",
      "\tat org.apache.hudi.io.HoodieAppendHandle.appendDataAndDeleteBlocks(HoodieAppendHandle.java:466)\n",
      "\tat org.apache.hudi.io.HoodieAppendHandle.doAppend(HoodieAppendHandle.java:431)\n",
      "\tat org.apache.hudi.table.action.deltacommit.BaseSparkDeltaCommitActionExecutor.handleUpdate(BaseSparkDeltaCommitActionExecutor.java:90)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:337)\n",
      "\t... 29 more\n",
      "Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[172.18.0.4:9866,DS-faffac4f-853d-4442-8119-3039a5546668,DISK]], original=[DatanodeInfoWithStorage[172.18.0.4:9866,DS-faffac4f-853d-4442-8119-3039a5546668,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1352)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1420)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1646)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1547)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:717)\n",
      "\n",
      "24/07/27 09:01:07 WARN WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path hdfs://namenode:9000/user/hive/warehouse/bizvault/test_hudi.  Falling back to direct markers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/27 09:01:07 WARN HoodieSparkSqlWriterInternal: Closing write client\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "insert into bizvault.test_hudi\n",
    "values (1, \"2\")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a5302ac-b71d-45de-8d82-805b4f9191f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+---+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|key|value|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+---+-----+\n",
      "|  20240727090100061|20240727090100061...|20240727090100061...|                      |441942c4-d488-4b0...|  1|    2|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "select * from bizvault.test_hudi\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50e57c-dc7d-44cf-82a6-06214b7de9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39309a39-aea4-40e3-a3cc-c01fa92e4a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644cd0b0-e6e6-4312-aea1-afb6667f5761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
