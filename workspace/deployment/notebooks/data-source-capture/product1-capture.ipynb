{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033605f3-bea0-4368-ae7c-fe1bca80f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from datetime import datetime, date\n",
    "import pytz\n",
    "\n",
    "# database connection\n",
    "DB_SERVER=os.environ['PRODUCT1_SERVER']\n",
    "DB_PORT=os.environ['PRODUCT1_PORT']\n",
    "DB_USER=os.environ['PRODUCT1_USER']\n",
    "DB_PASSWORD=os.environ['PRODUCT1_PASSWORD']\n",
    "DATABASE = \"product1\"\n",
    "\n",
    "# jdbc\n",
    "JDBC_DRIVER = \"org.postgresql.Driver\"\n",
    "JDBC_TYPE = \"jdbc:postgresql\"\n",
    "JDBC_URL = f\"{JDBC_TYPE}://{DB_SERVER}:{DB_PORT}/{DATABASE}\"\n",
    "\n",
    "# datetime format\n",
    "TIMEZONE = 'Asia/Ho_Chi_Minh'\n",
    "DATE_FORMAT = '%Y-%m-%d'\n",
    "DATETIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "def current_systime() -> datetime:\n",
    "    return datetime.now(pytz.timezone(TIMEZONE)).strftime(DATETIME_FORMAT)\n",
    "\n",
    "def spark_read_jdbc(\n",
    "        spark: SparkSession,\n",
    "        query: str,\n",
    "        driver: str = JDBC_DRIVER,\n",
    "        url: str = JDBC_URL,\n",
    "        user: str = DB_USER,\n",
    "        password: str = DB_PASSWORD\n",
    "    ) -> DataFrame:\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"driver\", driver)\n",
    "        .option(\"url\", url)\n",
    "        .option(\"user\", user)\n",
    "        .option(\"password\", password)\n",
    "        .option(\"query\", query)\n",
    "        .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff41750f-b107-4ee1-ae75-5981b3a8c3af",
   "metadata": {},
   "source": [
    "# init spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2affadf0-7c1b-4532-bb59-4ba5e4cfbf17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/08/18 12:23:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/18 12:23:30 WARN TransportClientFactory: DNS resolution succeed for spark-master/172.18.0.17:7077 took 5006 ms\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .appName(\"product1-capture\")\n",
    "    .config(\"spark.driver.memory\", \"1g\")\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse/default\")\n",
    "    .config(\"spark.jars\", \"hdfs://namenode:9000/user/hive/spark_jars/postgresql-42.7.3.jar\")\n",
    "    .config(\"spark.executor.cores\", 3)\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .config(\"spark.executor.instances\", 2)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9fe0b3-fbda-430a-9e2c-e20a80fb0a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gateway:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>product1-capture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffffa6be5a80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2c74a-b766-4014-9166-0ad81d2ca636",
   "metadata": {},
   "source": [
    "# create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61596de1-a3af-4b78-b7e5-eb2119473474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_list ['customer', 'service', 'period', 'trans', 'review']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "table_list = (\n",
    "    spark_read_jdbc(spark, f\"\"\"\n",
    "        SELECT table_name FROM information_schema.tables\n",
    "        WHERE table_schema='public'\n",
    "    \"\"\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "print(\"table_list\", table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aab95783-2b8a-4530-b31f-f164e56b925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_datetime 2024-08-17 16:58:25\n"
     ]
    }
   ],
   "source": [
    "checkpoint_datetime = current_systime()\n",
    "print(\"checkpoint_datetime\", checkpoint_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ebec1c7-8927-46d5-a581-62f4dd4b83bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema [('customer_id', 'bigint'), ('first_name', 'string'), ('last_name', 'string'), ('birth_date', 'date'), ('address', 'string'), ('phone_number', 'string'), ('email', 'string'), ('job_title', 'string'), ('updated_datetime', 'timestamp')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/17 09:58:26 WARN DropTableCommand: org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/user/hive/warehouse/staging/product1_customer.\n",
      "org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/user/hive/warehouse/staging/product1_customer.\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1419)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)\n",
      "24/08/17 09:58:26 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_table_query \n",
      "        CREATE EXTERNAL TABLE staging.product1_customer (\n",
      "            customer_id bigint, first_name string, last_name string, birth_date date, address string, phone_number string, email string, job_title string, updated_datetime timestamp\n",
      "        )\n",
      "        STORED AS ORC\n",
      "        LOCATION 'hdfs://namenode:9000/user/hive/warehouse/staging/topics/product1.public.customer'\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for table in table_list:\n",
    "    df = spark_read_jdbc(spark, f\"\"\"\n",
    "        SELECT * FROM {table}\n",
    "        WHERE updated_datetime <= '{checkpoint_datetime}'\n",
    "    \"\"\")\n",
    "    schema = df.dtypes\n",
    "    print(\"schema\", schema)\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS staging.{DATABASE}_{table}\")\n",
    "    create_table_query = f\"\"\"\n",
    "        CREATE EXTERNAL TABLE staging.{DATABASE}_{table} (\n",
    "            {', '.join([column + ' ' + type for column, type in schema])}\n",
    "        )\n",
    "        STORED AS PARQUET\n",
    "        LOCATION 'hdfs://namenode:9000/user/hive/warehouse/staging/topics/{DATABASE}.public.{table}'\n",
    "    \"\"\"\n",
    "    print(\"create_table_query\", create_table_query)\n",
    "    spark.sql(create_table_query)\n",
    "\n",
    "    # (\n",
    "    #     df.write\n",
    "    #     .format(\"orc\")\n",
    "    #     .mode(\"append\")\n",
    "    #     .save(f\"hdfs://namenode:9000/user/hive/warehouse/staging/{DATABASE}_{table}\")\n",
    "    # )\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7395861d-b509-4047-960a-a00e0662a2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+----------+-----------------+------------+------------------+-----------------+----------------+\n",
      "|customer_id|first_name|last_name|birth_date|address          |phone_number|email             |job_title        |updated_datetime|\n",
      "+-----------+----------+---------+----------+-----------------+------------+------------------+-----------------+----------------+\n",
      "|4          |Cuong     |Vo       |11510     |12 ABC, MNO, XYZ |01234       |cuongvo@gmail.com |Data Engineer    |1723911737000000|\n",
      "|5          |Cuong     |Vo       |11510     |12 ABC, MNO, XYZ |01235       |cuongvo1@gmail.com|Data Engineer    |1723911824000000|\n",
      "|6          |Cuong     |Vo       |11510     |12 ABC, MNO, XYZ |012356      |cuongvo2@gmail.com|Data Engineer    |1723911838000000|\n",
      "|1          |Vu        |Tran     |11378     |123 ABC, MNO, XYZ|0865937123  |nitsvutt@gmail.com|Data Engineer    |1723898668000000|\n",
      "|2          |The       |Duong    |11510     |132 ABC, MNO, XYZ|0865937124  |theduong@gmail.com|Software Engineer|1723907232000000|\n",
      "|3          |Dien      |Vo       |11510     |133 ABC, MNO, XYZ|0865937134  |dienvo@gmail.com  |Data Engineer    |1723910442000000|\n",
      "+-----------+----------+---------+----------+-----------------+------------+------------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read\n",
    "    .parquet('hdfs://namenode:9000/user/hive/warehouse/staging/topics/product1.public.customer')\n",
    "    .select(\"after.*\")\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a6b1d4d-a66a-42fd-8a65-833222337156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- before: struct (nullable = true)\n",
      " |    |-- customer_id: long (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |    |-- birth_date: integer (nullable = true)\n",
      " |    |-- address: string (nullable = true)\n",
      " |    |-- phone_number: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- job_title: string (nullable = true)\n",
      " |    |-- updated_datetime: long (nullable = true)\n",
      " |-- after: struct (nullable = true)\n",
      " |    |-- customer_id: long (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |    |-- birth_date: integer (nullable = true)\n",
      " |    |-- address: string (nullable = true)\n",
      " |    |-- phone_number: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- job_title: string (nullable = true)\n",
      " |    |-- updated_datetime: long (nullable = true)\n",
      " |-- source: struct (nullable = true)\n",
      " |    |-- version: string (nullable = true)\n",
      " |    |-- connector: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- ts_ms: long (nullable = true)\n",
      " |    |-- snapshot: string (nullable = true)\n",
      " |    |-- db: string (nullable = true)\n",
      " |    |-- sequence: string (nullable = true)\n",
      " |    |-- schema: string (nullable = true)\n",
      " |    |-- table: string (nullable = true)\n",
      " |    |-- txId: long (nullable = true)\n",
      " |    |-- lsn: long (nullable = true)\n",
      " |    |-- xmin: long (nullable = true)\n",
      " |-- op: string (nullable = true)\n",
      " |-- ts_ms: long (nullable = true)\n",
      " |-- transaction: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- total_order: long (nullable = true)\n",
      " |    |-- data_collection_order: long (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read\n",
    "    .parquet('hdfs://namenode:9000/user/hive/warehouse/staging/topics/product1.public.customer')\n",
    "    .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198fdf2-8426-49f4-b0ad-6107ce021084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ccf9b3-e175-4115-ae9d-82daed9691e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa2ecf-8a14-4e98-8fcc-dd4e04085684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
