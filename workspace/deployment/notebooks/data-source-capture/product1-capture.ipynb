{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "033605f3-bea0-4368-ae7c-fe1bca80f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from datetime import datetime, date\n",
    "import pytz\n",
    "\n",
    "# database connection\n",
    "DB_SERVER=os.environ['PRODUCT1_SERVER']\n",
    "DB_PORT=os.environ['PRODUCT1_PORT']\n",
    "DB_USER=os.environ['PRODUCT1_USER']\n",
    "DB_PASSWORD=os.environ['PRODUCT1_PASSWORD']\n",
    "DATABASE = \"product1\"\n",
    "\n",
    "# jdbc\n",
    "JDBC_DRIVER = \"org.postgresql.Driver\"\n",
    "JDBC_TYPE = \"jdbc:postgresql\"\n",
    "JDBC_URL = f\"{JDBC_TYPE}://{DB_SERVER}:{DB_PORT}/{DATABASE}\"\n",
    "\n",
    "# datetime format\n",
    "TIMEZONE = 'Asia/Ho_Chi_Minh'\n",
    "DATE_FORMAT = '%Y-%m-%d'\n",
    "DATETIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "def current_systime() -> datetime:\n",
    "    return datetime.now(pytz.timezone(TIMEZONE)).strftime(DATETIME_FORMAT)\n",
    "\n",
    "def spark_read_jdbc(\n",
    "        spark: SparkSession,\n",
    "        query: str,\n",
    "        driver: str = JDBC_DRIVER,\n",
    "        url: str = JDBC_URL,\n",
    "        user: str = DB_USER,\n",
    "        password: str = DB_PASSWORD\n",
    "    ) -> DataFrame:\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"driver\", driver)\n",
    "        .option(\"url\", url)\n",
    "        .option(\"user\", user)\n",
    "        .option(\"password\", password)\n",
    "        .option(\"query\", query)\n",
    "        .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff41750f-b107-4ee1-ae75-5981b3a8c3af",
   "metadata": {},
   "source": [
    "# init spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2affadf0-7c1b-4532-bb59-4ba5e4cfbf17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/08/17 06:11:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .appName(\"product1-capture\")\n",
    "    .config(\"spark.driver.memory\", \"1g\")\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse/default\")\n",
    "    .config(\"spark.jars\", \"hdfs://namenode:9000/user/hive/spark_jars/postgresql-42.7.3.jar\")\n",
    "    .config(\"spark.executor.cores\", 3)\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .config(\"spark.executor.instances\", 2)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fe0b3-fbda-430a-9e2c-e20a80fb0a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gateway:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>product1-capture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff52a554e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2c74a-b766-4014-9166-0ad81d2ca636",
   "metadata": {},
   "source": [
    "# create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61596de1-a3af-4b78-b7e5-eb2119473474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_list ['customer', 'service', 'period', 'trans', 'review']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "table_list = (\n",
    "    spark_read_jdbc(spark, f\"\"\"\n",
    "        SELECT table_name FROM information_schema.tables\n",
    "        WHERE table_schema='public'\n",
    "    \"\"\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "print(\"table_list\", table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aab95783-2b8a-4530-b31f-f164e56b925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_datetime 2024-08-17 14:08:31\n"
     ]
    }
   ],
   "source": [
    "checkpoint_datetime = current_systime()\n",
    "print(\"checkpoint_datetime\", checkpoint_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ebec1c7-8927-46d5-a581-62f4dd4b83bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema [('customer_id', 'bigint'), ('first_name', 'string'), ('last_name', 'string'), ('birth_date', 'date'), ('address', 'string'), ('phone_number', 'string'), ('email', 'string'), ('job_title', 'string'), ('updated_datetime', 'timestamp')]\n",
      "create_table_query \n",
      "        CREATE EXTERNAL TABLE staging.product1_customer (\n",
      "            customer_id bigint, first_name string, last_name string, birth_date date, address string, phone_number string, email string, job_title string, updated_datetime timestamp\n",
      "        )\n",
      "        STORED AS ORC\n",
      "        LOCATION 'hdfs://namenode:9000/user/hive/warehouse/staging/product1_customer'\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for table in table_list:\n",
    "    df = spark_read_jdbc(spark, f\"\"\"\n",
    "        SELECT * FROM {table}\n",
    "        WHERE updated_datetime <= '{checkpoint_datetime}'\n",
    "    \"\"\")\n",
    "    schema = df.dtypes\n",
    "    print(\"schema\", schema)\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS staging.{DATABASE}_{table}\")\n",
    "    create_table_query = f\"\"\"\n",
    "        CREATE EXTERNAL TABLE staging.{DATABASE}_{table} (\n",
    "            {', '.join([column + ' ' + type for column, type in schema])}\n",
    "        )\n",
    "        STORED AS ORC\n",
    "        LOCATION 'hdfs://namenode:9000/user/hive/warehouse/staging/{DATABASE}_{table}'\n",
    "    \"\"\"\n",
    "    print(\"create_table_query\", create_table_query)\n",
    "    spark.sql(create_table_query)\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"orc\")\n",
    "        .mode(\"append\")\n",
    "        .save(f\"hdfs://namenode:9000/user/hive/warehouse/staging/{DATABASE}_{table}\")\n",
    "    )\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7395861d-b509-4047-960a-a00e0662a2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198fdf2-8426-49f4-b0ad-6107ce021084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ccf9b3-e175-4115-ae9d-82daed9691e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa2ecf-8a14-4e98-8fcc-dd4e04085684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
